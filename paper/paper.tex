%\documentclass[10pt,twocolumn]{article}
\documentclass[10pt, conference]{IEEEtran}
%\IEEEoverridecommandlockouts
%\documentclass{article}
%\documentclass[12pt]{IEEEtran}
%\documentclass{elsart5p}
\usepackage{listings}
\usepackage{times}
\lstset{language=Python,
	frame=single,
	captionpos=b,
	frameround=tttt,
	basicstyle=\scriptsize,
	keywordstyle=\color{black}\bfseries,
                            % underlined bold black keywords
	identifierstyle=,           % nothing happens
	commentstyle=\color{white}, % white comments
	stringstyle=\ttfamily,      % typewriter type for strings
	showstringspaces=false}     % no special string spaces


% DFRWS Call Information:
% http://www.dfrws.org/2009/cfp.shtml
% Important Dates
% Submission deadline: March 16, 2009 (any time zone)
% Author notification: April 28, 2009
% Final draft due: May 19, 2009
% Pre-conference Workshops: August 16, 2009
% Conference dates: August 17-19, 2009

% Publication Criteria
% Research papers must be original contributions, not substantially
% duplicate previous work, and must not be under simultaneous
% publication review elsewhere. The review process will be
% ``double-blind'' (the reviewers will not know who the authors are, and
% the authors will not know who the reviewers are). Therefore, the
% version submitted for review should not contain the names or
% affiliations of the authors. When referring to their own previous
% work, authors should use the third person instead of the first person
% (i.e. ``Smith and Jones [2] previously determined...'' instead of ``We
% [2] previously determined..''). Authors are expected to present their
% work in person at the workshop and must have at least one registration
% per paper in order to be included in the proceedings.


% Make sure we know its a draft for now
\usepackage{draftwatermark}
\SetWatermarkFontSize{3cm}
\usepackage{fancyvrb}
%\documentclass[12pt]{article}
\usepackage{epsfig}
\usepackage[hypertexnames=false,bookmarksopenlevel=1,bookmarksopen,bookmarksnumbered,colorlinks,plainpages=false,linktocpage,linkcolor=black,citecolor=black,filecolor=black,urlcolor=black]{hyperref}
%\usepackage{natbib}
%\pagestyle{empty}
%\psdraft
%\baselineskip=20pt %Sets line spacing to 1 unit
%\bibliographystyle{elsart-num-names}
\usepackage{cite}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1:}]}{\end{trivlist}}

%\begin{frontmatter}
\newcommand{\omitt}[1]{}
\begin{document}
\title{Extending the Advanced Forensic Format to accommodate Multiple
  Data Sources, Logical Evidence, Arbitrary Information and Forensic Workflow}
\author{Blinded for review}
%\author{M. I. Cohen\footnote{scudette@gmail.com}, Simson Garfinkel, and Bradley Schatz}
%\author{M. I. Cohen}
%\address{M. I. Cohen is a Data specialist with the Australian Federal Police, Brisbane, Australia}
%\baselineskip=20pt %Sets line spacing to 1 unit
%\end{frontmatter}
%\thanks{M. I. Cohen is a Data specialist with the Australian Federal Police, Brisbane, Australia}
\maketitle

\begin{abstract}
Forensics analysis requires the acquisition and management of many
different types of evidence material, including individual disk
drives, RAID sets, network packets, memory images, and extracted
files. Often the same evidence is reviewed by several different tools
or examiners in different locations. We propose a backwards-compatible
evolutionary redesign of the Advanced Forensic Format an open,
extensible file format for storing and sharing of evidence, arbitrary
case related information and analysis results among different
tools. The new specification was designed to be simple to implement,
allowing the use of the well supported Zip File format specifications
for bit level file access.
\end{abstract}

\section{Introduction}
Unlike other kinds of evidence, digital evidence can be instantly lost
or corrupted if technical measures are not immediately taken to
preserve its existence and integrity. As a result, preserving digital
evidence is an important part of most digital
investigations\cite{carrier:event-based}.

In recent years there has been a steady and growing interest in the
actual file formats and containers used to store this digital
evidence. Early practitioners created exact bit-for-bit copies
(``mirror disks'') employed proprietary software systems
(e.g. \cite{safeback,ilook,encase}) for making and authenticating
``images'' of digital evidence. PyFlag\cite{pyflag} introduced a
``seekable gzip'' format that allowed disk images to be stored in a
form that was compressed but allowed the random-access to evidence
data necessary for forensic analysis. The \emph{sgzip} format does not
store any metadata at all.

The Advanced Forensic Format (AFF) expanded on this idea with a
forensic file format that allowed both data and arbitrary metadata to
be stored in a single digital archive\cite{garfinkel:aff}. The EWF
file format also provides for a small number of predefined metadata
fields to be stored within the file format.

Both of these approaches are limited in their expressiveness, by
nature of their (property, value) data model. The subject of the
properties is implicitly the single image contained within the
evidence container. Clearly an implicit subject metadata model is
inadequate for storing and managing multiple evidence sources
simultaneously.

The Sealed Digital Evidence Bags architecture of Schatz proposed an
approach to digital evidence containers that facilitated composition
of evidence and arbitrary evidence related information, through a
simple data model and globally unique referencing
scheme\cite{schatz:sdeb}. 

The AFF4 metadata model described in this work, is based on (subject,
property, value) statements. An explicitly defined subject (identified
by an AFF4 URN) allows one to not only associate traditional EWF style
metadata with a contained image, but to also define new information
entities representing both real world and virtual entities related to
the matter. For example, an AFF4 compliant hard disk acquisition tool
should, in addition to storing an image, store entities representing
and describing the tool itself, the operator of the tool, the user
supplied parameters supplied to the tool, and the drive being imaged.

Recently the open source AFF implementation library (AFFLIB) was
expanded to support both encryption and digital signatures of digital
evidence\cite{garfinkel:affcrypto}. Clearly, the implementation of
encryption and authentication is critical for forensic file formats.

This paper extends the previous work by adapting AFF to the multiple
heterogeneous data types that might arise in a modern digital
investigation, including data from multiple data storage devices, new
data types (including network packets and memory images), extracted
logical evidence, and forensic workflow. We do this in a manner that
is at once upwards compatible with AFF while adopting the AFF
bit-level specification so that AFF files can be processed using
standard non-forensic tools.  We call the new system AFF4, and use the
phrase AFF1 to refer to the legacy system developed by
Garfinkel.\footnote{Although Garfinkel never changed the AFF bit-level
specification, Garfinkel released AFFLIB implementations with major
version numbers 1, 2 and 3. We therefore call our system AFF4 to avoid
confusion.}

\begin{figure}
\fbox{
\begin{minipage}{.95\linewidth}
\begin{itemize}
\item The ability to store evidence from one or more physical
  devices.
\item Schema allowing evidence files to contain memory
  images, network packet intercepts, Netflow data, and extracted
  files.

\item Direct encoding of arbitrary mapping transformations of one or
more sources to allow multiple logical views. This allows 
RAID volumes to be automatically reassembled from evidence files made
of the physical RAID devices. Maps can be created using Sleuthkit and
stored in the evidence file, thereafter allowing arbitrary programs to directly
read individual files stored in a disk image.

\item A revised compression system that allows individual chunks
  within a 16MB segment to be decompressed without decompressing the
  entire segment.

\item Support for Global Distributed Evidence Management.
\end{itemize}
\end{minipage}}
\caption{Specific design goals for the next generation forensic file
  format.}
\end{figure}
\section{The Need for an Improved Forensic Format}

AFF1's flexibility came from the fact that forensic data and metadata
is stored as arbitrary name/value pairs called \emph{segments}. For
example, the first 16MB of a disk image is stored in a segment called
\texttt{page0}, the second 16MB in a segment called \texttt{page1},
\emph{etc.} Because of this flexibility, it was relatively easy for
Garfinkel to extend AFF1 to support encryption, digital signatures, and
the storage of new kinds of metadata such as chain-of-custody
information\cite{garfinkel:affcrypto}. 

\subsection{AFF Limitations}
After imaging literally thousands of devices using Garfinkel's AFF1
tools and adding support for AFF1 to numerous open source programs, we
observed numerous problems in the underlying standard
and Garfinkel's AFFLIB implementation:

\begin{itemize}
\item While AFF1's design stores a single disk image in each evidence
  file, modern digital investigations typically involve many seized
  computers or pieces of media. 

\item The data model of AFF1 enabled storing metadata related to the 
  contained image as (property, value) pairs. This data model does
  not, however, support expressing arbitrary information about more
  than one entity.

\item AFF1 has no provision for storing memory images or intercepted
  network packets.

\item AFF1 has no provisions for storing extracted files that is
  analogous to the EnCase ``Logical Evidence File'' (L01) format, or
  for linking evidence to web pages.

\item AFF1's encryption system leaks information about the contents of
  an evidence file because segment names are not encrypted.

\item AFF1's default compression page size of 16MB can impose significant overhead
  when accessing NTFS Master File Tables (MFT), as these structures
  tend to be highly fragmented on systems that have seen significant
  use.

\item Although the AFF1 specification calls for a ``table of contents'' similar
  to the Zip\cite{zip-format} ``central directory'' that is stored at
  the end of AFF files, Garfinkel never implemented this directory in
  the publicly released AFF1 implementation, AFFLIB. As a result,
  every header of every segment in an AFF file needs to be read when a
  file is opened. It practice this can take 10--30 seconds the first
  time a large AFF file is opened. (Once the file is opened, the
  sectors corresponding to the segment headers are stored in the
  computer's disk cache.)

\item AFF1's bit-level specification is essentially a simple container
  file specification. Given the that there are other container file
  specifications that are much more widely supported with both
  developer and end-user tools, it seemed reasonable to migrate AFF
  from its home-grown format to one of the existing standards.

\end{itemize}

\section{Global Distributed Evidence Management}
While Garfinkel designed AFF for use on a single machine that could
both image evidence and perform analysis, the main use of AFF in
practice has been in distributed environments in which imaging and
analysis takes place in multiple locations and is performed by
multiple individuals. This is a state of affairs which mirrors common
practice in the field. 

Global distributed evidence management requires
more than simply tracking the movement of disk images: it requires
approaches for sharing evidence to multiple disconnected
evidence, allowing offline work, and then seamlessly recombining the
work products of the analysts in a third security domain.

Managing evidence in a globally distributed system requires the use of
globally unique identifiers. AFF1 assigns
each piece of evidence a unique 128-bit identifier called a GID but did
not make it clear when this identifier should be changed and when it
should remain the same. 

Consider the typical usage scenario depicted in Figure \ref{usage}, of
an file which contains a disk image. This volume is distributed to two
independent analysts, Alice and Bob. Alice may find and extract
individual files, while Bob may correlate information in the evidence
file with other data that is available on departmental
servers. Although in some environments Alice and Bob may be able to
work on a shared file that is located on a server, in other
environments there will not be sufficient bandwidth or
connectivity. Instead, each analyst will be required to store the
information in their own evidence file; these files will then be
recombined at a later point in time.

In this case they can each create a new volume which extends to
original volume and save their analysis on this new volume. Now they
only need to share this new volume with other analysts who also have a
copy of the old volume to interchange their findings.

This is made possible because each volume is independent of one
another, but can still be recombined into a bigger evidence set. 

\begin{figure}[tb]
  \begin{center}
  \mbox{\epsfxsize=0.6\columnwidth \epsffile{usage1.eps}}
  \caption{A typical usage scenario. Both Alice and Bob receive an AFF
  volume but work independently. Rather than modifying the volume,
  they each create their own local volumes and save their results into
  those files. They can now exchange the smaller new volumes and
  effectively merge their results into the same AFF set when they are finished.}
  \label{usage}
  \end{center}
\end{figure}

\subsection{Migration from AFF1 to AFF4}
We have addressed these issues in a number of ways. First, we have
extended the AFF schema to support multiple forensic images (which are
now called \emph{streams}). It is therefore possible now to store
multiple related streams in the same volume, or to acquire mutiple
streams at the same time (e.g. network and disk data).

We replaced AFF's ``bit-level'' format with an improved format that is
more flexible and has better existing support, namely the Zip64 file
format is not adopted as a default storage format. This enhanced our
compatibility with other tools and simplifies implementations for
compatible systems.

We have also defined an information model which addresses uniquely
identifying evidence data, expressing arbitrary information and
integrating information and image data from multiple evidence volumes
at a time. This allows us to extend evidence collection and management
to a truely universal scale - allowing creation and management of a
large corpus.

We have also adopted AFFLIB to implement the new schema and file
format, which will allow existing AFF-aware programs to access images
written in either the old or the new format without modification. We
are extending the AFFLIB API to support the new information mode.

We call the result of our work effort AFF4.

\section{Introducing AFF4}

This section discusses the AFF4 terminology. The AFF4 design is object
oriented, in that a few generic objects are presented with externaly
accessible behaviour. We discuss a number of implementations of these
high level concepts and show how these can be put together in common
usage cases.

\begin{itemize}
\item \emph{An AFF Object} is the basic building block of our
file format. AFF Objects have a globally unique name (URN) as described in
\cite{RFC1737}. The name is defined within the aff2 namespace, and is
made unique by use of a unique identifier generated as per
RFC4122\cite{RFC4122}.

\item \emph{A Relation} is a statement about an AFF Object which
expresses some property of the object. The relation comprises of a
triple of (Subject, Attribute, Value). All metadata is reduced to the
triple notation.

\item \emph{An Evidence Volume} is a type of AFF Object which is
responsible for providing storage to AFF segments. Volumes must
provide a mechanism for storing and retrieving segments by their
URN. We discuss two volume implementations below, namely the {\em
Zip64 based volume} and the {\em Directory} based volume.

\item \emph{A segment} is a single unit of data written to a volume. AFF4
  segments have a \emph{segment name} provided by their URN, a
  \emph{segment timestamp} in GMT, and the \emph{segment contents}.

\item \emph{A stream} is an AFF Object which provides the ability to
seek and read random data. Stream objects implement abstracted
storage, but must provide clients the stream like interface. For
example, we discuss the {\em Image stream} used to store large images,
the {\em Map stream} used to create transformations and the {\em
Encrypted stream} used to provide encryption.

\item \emph{A Target Reference} is a way of referencing objects by use
of a Uniform Resource Identifier (URI). The URI can be another AFF
Object URN or may be a more general Uniform Resource Locator (URL),
such as for example a HTTP or FTP object. This innovation allows
objects in one volume to refer to objects in different volumes,
facilitating data fusion and cross referencing.

\item \emph{The Resolver} is a central data store which collects and resolves
attributes for the different AFF Objects. The Resolver has universal
visibility of objects from all volumes, and therefore guides
implementations in resolving external references. We discuss the
Resolver in detail in Section \ref{resolver}.
\end{itemize}

\section{Metadata and the Universal Resolver}
\label{resolver}
One of the fundamental tasks of forensics is in ensuring the
provenance of any particular piece of digital evidence, for example by
employing a chain of custody process. To assist in this, acquisition
methodologies emphasise the process of individuating evidence, and
storing it in a manner that it is easily identifiable.

Management of evidence requires on effective identififaction, with
practitioners currently employing acquisition time metadata such as
case identifiers and description fields in the EWF file format; file
and directory naming schemes, and labeling of evidence container hard
drives. Evidence may also be referred to by external means in an
inconsistent way. For example, in an investigator's case note a disk
image may be referred to by the name of the suspect (e.g. Joe's hard
disk), the case number or dates.

Such individuation schemes may be, however, problematic when
automatically managing evidence. For example, at acquisition time a
suitably unique individuator may not be selected. If that occured, at
analysis time evidence container files may need to be renamed to avoid
name collisions.

The AFF4 design adopts a scheme of globally unique identifiers for
identifying and referring to all evidence. We define an AFF4 specific
URN namespace\cite{RFC1737} for our naming scheme (\emph{urn:aff4}).
URNs can then be made unique by use of a unique identifier generated
as per RFC4122\cite{RFC4122}. For example, an AFF4 URN might be
\emph{urn:aff4:bcc02ea5-eeb3-40ce-90cf-7315daf2505e}

In this way we can identify uniquely any evidence without the change
for name collisions. Once a universal identification regime is
created, it is now possible to collect metadata about each object. The
AFF4 model treats metadata as an abstract concept which may exists
independently from the data itself. We term {\em metadata} to be a set
of statements about objects, written in triple notations (Subject,
Attribute, Value), where {\em Subject} is the URN of the object the
statement is made about. Using this system we are able to store
arbitrary attributes about any object in the AFF4 universe.

The AFF4 design extends beyond the management of a single volume,
stream or image to a universal system for managing data of many
types. This necessarily means that a single running instance is
generally unable to have visibility of the entire AFF4 universe. For
example, if a volume is opened which contains a Map Stream targeting a
stream stored in a different volume, its not generally possible to
tell in which volume the stream exists, and where that volume is
actually stored.

To provide this global visibility of metadata we define a central
metadata management entity, named the {\em Universal Resolver}. The
Universal Resolver contains all the metadata about the AFF4 universe,
that is to say it is able to resolve queries about any attribute about
any URN in the universe.

Although the resolver has complete visibility of all attributes, it is
still useful to store metadata within the volume itself, particularly
data pertaining to the volume itself. If we did not store the metadata
within the volume itself, then the volume would not be accessible to
implementations which do not have this metadata.

To this end we define a way for serializing metadata statements (or
triples) into a standard format which implementations can load into
their respective resolvers when parsing the volume. Triples can be
stored in segments having a URN ending with ``{\em properties}''. The
AFFLIB implementation loads these segments automatically into the
Universal Resolver. 

Triples are stored within the properties segment one per line, with
the subject URN (encoded according to RFC1737), followed by whitespace
and the attribute name. This is then followed by the equal sign and
the UTF8 encoding of the value. An example properties file for an
Image Stream is shown in Listing \ref{image_metadata}.

It is important to stress that the properties file is simply a
serialization of statements into volume segments. The statements may
exist without being stored in a volume (for example, being stored on
an external SQL server). Alternatively, these statements may be stored
in some other way inside or outside the volume (e.g. SQLite
databases).

When the volume is loaded, the AFFLIB implementation automatically
loads any properties files and populates its Universal Resolver with
the information visibile to it. AFFLIB provides a mechanism to use an
external resolver stored on a MySQL database to provide a Universal
Resolver that shares information between different instances on the
same network.

Each URN within the AFF4 universe must have an ``\texttt{aff4:type}''
attribute to denote the type of the Object.

\begin{lstlisting}[
	caption=Example properties files for several AFF4 objects (URNs are
shortened for illustration).,
	label=image_metadata,
	float=tb,
	 ]
Directory Volume:
  urn:aff4:f901be8e-d4b2 aff4:stored=http://../case1/
  urn:aff4:f901be8e-d4b2 aff4:type=directory

ZipFile Volume:
  urn:aff4:98a6dad6-4918 aff4:stored=file:///file.zip
  urn:aff4:98a6dad6-4918 aff4:type=zip

Image Stream:
  urn:aff4:83a3d6db-85d5 aff4:stored=urn:aff4:f901be8e-d4b2
  urn:aff4:83a3d6db-85d5 aff4:chunks_in_segment=256
  urn:aff4:83a3d6db-85d5 aff4:chunk_size=32k
  urn:aff4:83a3d6db-85d5 aff4:type=image
  urn:aff4:83a3d6db-85d5 aff4:size=5242880

Map Stream:
  urn:aff4:ed8f1e7a-94aa aff4:target_period=3
  urn:aff4:ed8f1e7a-94aa aff4:image_period=6
  urn:aff4:ed8f1e7a-94aa aff4:blocksize=64k
  urn:aff4:ed8f1e7a-94aa aff4:stored=urn:aff4:83a3d6db-85d5
  urn:aff4:ed8f1e7a-94aa aff4:type=map
  urn:aff4:ed8f1e7a-94aa aff4:size=0xA00000

Link Object:
  map aff4:target=urn:aff4:ed8f1e7a-94aa
  map aff4:type=link
\end{lstlisting}

\section{Volumes}
The volume object is responsible for providing storage for
segments. Segments are stored and retrieved using their URNs. We
describe two different implementations of volume objects, namely the
{\em Directory Volume} and the {\em ZipFile Volume}. It is possible to
convert from one implementation to another easily, without effecting
any external references. This is because external references are to
the Volume URN itself, so its safe to convert from one representation
to another. In this way its possible for example, to unzip a Zip64
volume into a directory and vice versa.

\subsection{Directory Volumes}
The Directory Volume is the simplest type of volume. It simply stores
different segments based on their URNs in a single directory. Since
some filesystems are unable to represent URNs accurately (e.g. Windows
has many limitations on the types of characters allowed for a
filename), the Directory Volume encodes URNs according to RFC1738
\cite{RFC1738}; non-printable characters are escaped with a \%
followed by the ASCII ordinal of the character.

The Directory Volume uses the {\em aff4:stored} attribute to provide a
base URL. The URL for each segment is then constructed by appending
the escaped segment URN to the base URL. Note that there is no
restriction on what type of URL this can be, so it may be a location
on a filesystem (e.g. {\em file://some\_directory/}) or a location on a
HTTP server (e.g. {\em http://intranet.server/some\_path}). In this
way its possible to move the entire volume from a filesystem to a web
server transparently.

The Directory Volume stores its own URN in a special segment named
``{\em \_\_URN\_\_}'' at the base of the directory.

\subsection{Zip64 Volumes}
For AFF4, we have changed the default volume container file format to
Zip64\cite{zip-format}. There are many reasons for this decision:

\begin{itemize}
\item There is already wide support for the Zip and Zip64 formats. By
  migrating to these formats, we can take advantage of the rich number
  of user and developer tools already available. The volume may be
  inspected using any number of commercial zip application
  (e.g. Windows Explorer natively supports Zip files).
 
\item Zip64 already includes support for the ``table of contents''
  envisioned but never implemented by Garfinkel; Zip64 calls this
  table the ``Central Directory.'' It is written at the end of the
  Zip64 volume.

\item Zip64 libraries are readily available making proprietary implementations of
interfaces to the AFF4 volume format simple to write. For example, a
simple python program to dump out an Image stream
(section~\ref{image_stream}) is illustrated in Listing
\ref{python_code}.

\end{itemize}

\begin{lstlisting}[
	float=tb, label=python_code, caption=Sample Python code to
	dump out an Image Stream. As can be seen the chunk index
	segment is used to slice the data segment into chunks. The
	chunks are decompressed and written to the output file.]
volume=zipfile.ZipFile(INPUT_FILE)
outfd = open(OUTPUT_FILE,"w")
count = 0

while 1:
    idx_segment = volume.read(STREAM+"/%08d.idx" % count)
    bevy = volume.read(STREAM+"/%08d" % count)
    indexes = struct.unpack("<" + "L" * 
	(len(idx_segment)/4), idx_segment)

    for i in range(len(indexes)-1):
	chunk = bevy[indexes[i]:indexes[i+1]]
        outfd.write(zlib.decompress(chunk))

    count += 1
\end{lstlisting}

Figure~\ref{zip_structure} shows the basic structure of a Zip
archive. As can be seen, the archive consists of a {\em Central Directory} (CD)
located at the end of the archive. The CD is a list of pointers to
individual {\em File header} structures located within the body of the
archive. Headers are then followed by the file data, after it has been
compressed by the appropriate compression method (as specified in the
header). Each archived file is optionally followed by a {\em Data
Descriptor} describing the length and CRC of the archived file. Using
the data descriptor field allows implementations to write archives
without needing to seek in the output file. This allows Zip files to
be written to pipes for example, sending an image over the network
using netcat or ssh.

\begin{figure}[tbp]
  \begin{center}
  \mbox{\epsfxsize=0.8\columnwidth \epsffile{zip_structure.eps}}
  \caption{The basic structure of a Zip archive. Also can be seen how
new archive members are added to an existing Zip File. The
Central Directory gets overwritten by the new member, and a new
Central Directory is written on the end.  }
  \label{zip_structure}
  \end{center}
\end{figure}

It is important to note that AFF4 only requires that the volume be
capable of storing multiple named segments of data. Although our AFF4
implementation uses the Zip64 file format as an underlying storage
mechanism, our system also supports legacy AFF1 volumes as well as
segments that are stored in other storage systems---for example,
Amazon.com's Simple Storage System (S3)\cite{s3-aws-home-page-money}.

\subsection{Unused Zip64 Features}
There are a number of aspects in the Zip file format specifications
which we do not utilize.
\begin{itemize}
\item We ignore Zip64's built-in support for splitting
archives into multiple Zip files. Instead, our implementation treats
each volume as a complete and stand-alone Zip file. The AFF4
implementation then considers the segments contained within as
belonging to the universal collection. We provide the ability to split
a stream across volumes automatically, as each segment may be stored
in a different volume, and the Universal Resolver is used to find it.

\item Although Zip64 also defines encryption and authentication
extensions, we do not use them due to the restrictions imposed on
their use and because they lack the functionality that is important
for a forensic user. Instead, we use AFF1's digital signature
facilities for integrity and non-repudiation, and we introduce a new
stream based encryption scheme for ensuring data privacy
(Section~\ref{crypted_stream}).

\end{itemize}

\subsection{Implementing Zip64 in AFFLIB}
Although there are numerous Zip implementations available
today, we have created our own implementation using a combination of
available open source technology and our own novel contributions. We
are in the process of integrating this code into the AFFLIB library
and performing extensive regression testing. 

There are many reasons to develop our own Zip64 implementation for
AFF4:

\begin{itemize}
\item Most Zip implementations do not implement the Zip64
  extensions. These extensions are required to support Evidence
  Volumes larger than 2GB.

\item Simple Zip implementations might rescan the
  Central Directory for each segment request. Since in practice there
  can be a large number of segments in a volume, it is advisable to
  have a Zip64 implementation that is optimized to storing thousands
  (or even hundreds of thousands) of segments in an efficient data
  structure. In fact our implementation uses the Universal Resolver to
  store the parsed central directory information, which means that in
  most cases we dont even need to scan the Central Directory at all.

\item While the Zip specification duplicates data found in the Central
  Directory entry in each File Header (such as filename, size, CRC
  etc), many implementations that we have examined only populate this
  information in one of these places. In the interest of robustness,
  we wanted to ensure that data stored in both locations would be
  populated to allow recovery of at least \emph{some} evidence that
  might exist in damaged volumes. If the central directory is lost, it
  is possible to scan through the volume, and locate all the Zip64
  file headers. Then its possible to repair and reconstruct the
  central directory.

\item Our implementation supports simultaneous access by multiple
  readers and writers. Since our system requires all metadata to be
  shared through the Universal Resolver, this lends itself to
  providing Universal Locking on a per Object basis. So for example,
  if one process wants to add a new segment into a Zip volume, they
  can lock it via the Resolver, add the segment and unlock the volume
  object in the resolver, stopping concurrent access by other
  programs, even on different machines.
\end{itemize}

\section{Streams}
The Stream system provides random access to an abstract representation
of a body of data. Our implementation allows the segments in a stream
to be operated on as if they were a single file by supporting the
traditional POSIX-like functionality of
\texttt{open()}, \texttt{seek()}, \texttt{write()}, and
\texttt{read()}. All streams also have a ``\texttt{size}'' attribute
to denote the last byte addressable within the stream. This is
required in order to support the POSIX \emph{whence} attribute which
may require seeking from the end of the stream.

The following sections describe a number of types of streams. It is
important to note that clients of our implementation do not care how a
particular stream is implemented. Streams are opened by their URNs,
and the library itself ensures they provide the Stream interface. So
for example, users do not care if a stream is a Map Stream or an Image
Stream---the interface provided is the same.

\subsection{The Image Stream}
\label{image_stream}
The AFF4 \emph{Image Stream} stores a single read-only forensic data
set. For example, this stream might contains a hard disk image, a
memory image or a network capture (in PCAP format). Image streams have
an \texttt{aff4:type} attribute of \texttt{image}.

Storage for the data is done by using multiple data segments stored on
various volumes. Data segment URNs are derived by appending an 8
digit, zero padded decimal integer representation of an incrementing
id to the stream URN (e.g. ``urn:aff4:83a3d6db-85d5/00000032''). Each
data segment is called a bevy and stores a number of compressed chunks
back to back. 

The chunk index segment is a segment containing a list of relative
offsets to the beginning of each chunk within the bevy. The chunk
index segment URN is derived by appending the bevy URN with
``.idx''. This is illustrated in Figure \ref{image_stream_bevy}.

\begin{figure}[tb]
  \begin{center}
  \mbox{\epsfxsize=0.6\columnwidth \epsffile{image_segments.eps}}
  \caption{The structure of Image Stream Bevies. Each bevy is a
collection of compressed chunks stored back to back. Relative chunk
offsets are stored in the chunk index segment.}
  \label{image_stream_bevy}
  \end{center}
\end{figure}

Image streams specify the \texttt{chunk\_size} attribute, as the
number of image bytes each chunk contains (typically chunk sizes are
32kb). Also specified is the \texttt{chunks\_per\_segment} attribute
which specifies how many chunks are stored in each bevy. Each chunk is
compressed individually using the zlib compress algorithm. This
general structure of storing chunks within larger segments is similar
to the technique used by the Expert Witness file format (EWF) used by
EnCase\cite{encase-3.0} and implemented by the open source
libewf\cite{libewf} package. The advantages of keeping smaller sized
chunks is the better match between requested size and the minimum size
required for decompression. This means that less data is needed to be
decompressed unnecessarily.

%% FIXME - hashing may apply to any segment not just bevies
%Bevies can optionally be individually hashed, allowing for detection
%of modifications. The attribute \texttt{hash\_type} specifies the type
%of hash used (implementations will typically support md5,sha1 and
%sha256, but can define others). The hashes are stored in any number of
%segments within the stream, of arbitrary name. The hash segment names
%is provided by using the \texttt{hash\_name} stream attribute.

%Hashes are calculated in an identical way to that defined by AFF1's
%crypto layer\cite{garfinkel:affcrypto}---that is, the full name of the
%segment (including stream component) is followed by a single NULL
%byte, followed by the (uncompressed) content of the chunk. The
%inclusion of the segment name ensure that even identical chunks in
%content will have a different hash and prevents reordering attacks.

%Each hash segment is a list of a chunk id encoded in 32 bit
%little endian integer format followed by the hash (the length of which
%depends on the hash itself). This approach is illustrated in Figure
%\ref{hash_index}. 

%Hash segments can contain any number of chunk hashes in them. It is
%recommended that a hash segment be created when a volume is closed to
%cover all the chunks within that volume. This allows for hashes to be
%verified even if some volumes are missing. The AFFLIB implementation
%may update a hash of a chunk previously set by an earlier hash
%segment---this would be required when a chunk is updated. The hash
%segment can then be signed to assure the integrity of the updated segment.

%\begin{figure}[tb]
%  \begin{center}
%  \mbox{\epsfxsize=0.6\columnwidth \epsffile{hash.eps}}
%  \caption{The structure of a hashing segment. The attributes {\em
%  hash} and {\em hash\_type} declare the hash segment. The segment
%  contains a list of integer encoded chunk id followed by an unencoded
%  hash for each chunk.}
%  \label{hash_index}
%  \end{center}
%\end{figure}

%This design differs from AFF1, which creates a separate segment for
%each hash. AFF4 uses headers that are more verbose than the AFF
%segment headers. Therefore, creating a new segment for each hash value
%would result in too much overhead: Packing the hashes into a few
%segments it is possible to reduce this overhead to a minimum.

\subsection{The Map Stream}
\label{map_stream}
Linear transformations of data are commonplace in forensic
analysis. For example, a file is often simply a collection of bytes
drawn from an image, while a TCP/IP stream is simply a collection of
payloads from selected network packets. Sometimes the same data may be
viewed in a number of ways---for example a Virtual Address Space is a
mapping of the Physical Address Space through a page table
transformation.  Zero Storage Carving\cite{Meijer2006} is a way of
specifying carved files in terms of a sequence of blocks taken from
the image; Cohen extended this concept to an arbitrary mapping
function\cite{1363239,Cohen2007} which can be used to describe
arbitrary mappings of carved files within a single image.

In this work we extend the mapping function concept to allow a single
map to draw data from arbitrary streams (called {\em targets}). This
transform is implemented via the {\em Map stream}.

The mapping function is described in a segment named by appending
``/map'' to the stream URN. The segment data consists of a series of
lines, each containing a stream offset, followed by a comma, and a
target offset. Offsets are encoded using decimal notation, and are
followed by a target URN.

Denoting the stream offset by $x$, and the target offset by $y$, the
Map specifies a set of points $(X_i,Y_i,T_i)$. Read requests for a
byte at a mapped stream offset $x$ can then be satisfied by reading a
byte from target $T_i$ at offset $y$ given by:
\begin{eqnarray}
y = (x - X_i) + Y_i & &
\forall x \in \left [X_i, X_i+1 \right )
\end{eqnarray}

For example, consider the following map:
\begin{lstlisting}
0,0,urn:aff4:83a3d6db-85d5
4096,10000,urn:aff4:f901be8e-d4b2
8192,5000,urn:aff4:83a3d6db-85d5
\end{lstlisting}

To read this stream we satisfy read requests of offsets between 0 and
4095 in the stream from offset 0 to offset 4095 in
\emph{urn:aff4:83a3d6db-85d5}. Requests for bytes between 4096 and 8191 are
fetched from \emph{urn:aff4:f901be8e-d4b2} from offset 10000. Finally
bytes after 8192 (until the specified size of the stream) are fetched
from offset 5000 in \emph{urn:aff4:83a3d6db-85d5}.

In order to efficiently express periodic maps such as those found in
RAID arrays, the Map stream may be provided with two optional
parameters a {\em target\_period} ($T_p$), and {\em stream\_period}
($S_p$). If specified, the above relation becomes:
\begin{eqnarray*}
p &:=& floor\left (\frac{x}{S_p} \right) \\
x' &:=& mod(x ,S_p)  \\   \label{eq:no1}
y &:=& (x'-X_i) + Y_i + p \times T_p
\end{eqnarray*}

Where $mod$ is the modulus function and $floor$ signifies integer
division. For example consider Listing~\ref{map}, which corresponds to a 3
disk RAID-5 array.

\begin{lstlisting}[
	float=tb, caption=A Map stream that corresponds to a 3 disk
	RAID-5 array. The targets are URNs for the respective
        disks. Note that map coordinates are given in multiples of 
	block size.,
	label=map, language=]
aff4:block_size=64k 
aff4:stream_period=6 
aff4:target_period=3

0,0,disk1
1,0,disk0
2,1,disk2
3,1,disk1
4,2,disk0
5,2,disk2
\end{lstlisting}

%\Subsection{The Overlay Stream}
%The Overlay Stream Is A Variant Of The {\Em Image} Stream That
%Provides A Layer Of Indirection Between Requests Made For Specific
%Bytes (Or Chunks Or Bevys) And The Actual Location Where The Requested
%Information Is Stored. The Main Utility Of The Overlay Stream Is In
%Providing Transparent Access To Legacy Forensic Formats, Such As Ewf.

%To Access These Legacy File Formats We Only Need To Have An Index Of
%Each Chunk Offset, And Then Directly Use The Overlayed Files.  The {\Em
%Ewf2aff} Tool Provides Such An Overlay Feature For Ewf Volumes By
%Employing Libewf To Scan The Internal Data Structures.

%Overlay Stream Require That A \Texttt{Chunk\_Size} Attribute Be Specified. In
%Addition The {\Em Compression\_Type} Attribute May Be Specified. The
%Attribute Should Contain An Integer Corresponding To A Suitable Zip
%Compression Scheme (E.G. Deflate). If Not Specified It Is Assumed
%That No Compression Is Used.

%The ``Target'' Stream Attribute May Be Specified Multiple Times And
%Refer To External Files (See Section \Ref{Target_Reference}). There
%Can Be A Number Of Overlay Segments Named ``Stream Name/Overlay.00''
%Which Are All Merged To A Single Overlay. If More Than One Overlay Is
%Present, The Attribute {\Em Overlays} Must Be Specify How Many.

%The Overlay Segment Contains A Series Of Lines With Comma Delimited,
%Decimal Encoded, 64 Bit Integers Representing Chunk Number, Target
%Offset, Target Length And Target Number. Target Number Is An Index
%Into The Targets Defined In The Attributes (Count Starts With Zero).
%Chunks Are Then Fetched From The Specified Target And Decompressed If
%Necessary.

%It Is Possible To Refer To The Current File By Providing A Target Of
%``File://.''. This Can Be Useful For Modifying An Existing Ewf File To
%Be A Aff4 File. Since Zip Files Are Normally Read From The End Of The
%File, But Ewf Are Both Read From The Front Of The File, It May Be
%Possible To Append A Aff4 Central Directory To The End Of An Ewf File
%Without Interfering With The Overlayed File. In This Case The
%Converted File Can Still Be Used As An Ewf File Without Change, But
%Could Also Be Used As An Aff4 File.

%In Image Streams Some Chunks May Compress Very Well. In That Case, The
%Overheads Introduced By The Zip File Header For Each Chunk In The
%Image Stream May Become Unacceptably High. In That Case Its Possible
%To Use An {\Em Overlay Stream} To Coalesce Chunks Into Larger
%Segments. This Approach Is Illustrated In Figure \Ref{Overlay}.

%\Begin{Figure}[Tb]
%  \Begin{Center}
%  \Mbox{\Epsfxsize=0.6\Columnwidth \Epsffile{Overlay.Eps}}
%  \Caption{The Use Of An Overlay Stream To Overlay An Image With
%Coalesced Chunks.}
%  \Label{Overlay}
%  \End{Center}
%\End{Figure}

%In The Above Figure A Segment Is Written To The Aff4 Volume Which
%Contains A Number Of Chunks Back To Back. An Overlay Stream Then Uses
%Direct References To Each Compressed Chunk Within The Coalesced
%Segment. The Target Is Then Specified As The Segment Name. Although
%This Technique Is Valid, It Is Generally Discouraged Since It Breaks
%The Appealing Simplicity Of The Standard Image Stream.  It Is No
%Longer Possible To Simply Unzip All The Chunks And Concatenate Them
%Together To Recover The Original Image.

\subsection{The HTTP Stream}
Arguably the most ubiquitous protocol for information sharing is the
HTTP protocol\cite{HTTP_RFC}. The protocol features mature
authentication and auditing and is fast and easy to set up with
numerous web server implementations available on the market. The HTTP
protocol is also designed to operate across a wide range of network
architectures and is therefore more deployable than traditional file
sharing protocols.

For these reasons it is desirable to allow the HTTP protocol to be
used in facilitating the sharing of evidence files between
investigators. Luckily, the HTTP protocol fits naturally within the
URN based scheme adopted by AFF4, since the HTTP Universal Resource
Locator (URL) scheme is a subset of the URN scheme.

For this reason, URLs may be used interchangeably with a URN within
the AFF4 universe. For example, the \emph{aff4:stored} attribute of a
volume may be specified as a URL
(e.g. \emph{http://intranet/123453/}).  AFFLIB provides transparent
support for HTTP and FTP URLs by means of the curl HTTP
library\cite{libcurl}. The HTTP Stream, therefore satisfies read
requests by making HTTP requests to the web server. We use the
\emph{Content-Range} HTTP header to request exactly the byte range the
client is interested in. This allows efficient network transport as we
do not need to download unnecessary data, we just request exactly the
data the client application requires.

Currently the HTTP Stream is read only, but in future we anticipate
write support to be implemented, for example, using the WebDav
extensions to HTTP\cite{webdav-rfc}. The HTTP stream also supports the
File Transfer Protocol (FTP) and HTTPS (Secure Sockets Layer---SSL)
protocols transparently. This support is provided automatically by the
curl library.

\subsection{Encrypted Streams}
\label{crypted_stream}
Encryption is an important property in an evidence file format. In
particular, multiple streams may be present in the file set, and often
different access levels are desired. For example, for evidence set
containing both network captures and disk images it may be desirable
to limit access to streams based on legal authorizations, even though
the same set is distributed to a number of people.

Although the Zip64 standard specifies encryption, it is not suitable
for our purposes since it encrypts each segment separately, and does
not specify a sufficiently flexible scheme (e.g. support for PKI or
PGP keys). Segment based encryption may lead to information leakage
when segments are compressed, as the uncompressed size of the segment
may be deduced.

AFF4 therefore introduces a new encryption scheme, the Encrypted
Stream.  The Encrypted Stream provides transparent encryption and
decryption onto a single target stream. The target stream actually
stores the encrypted data, and read requests from the stream are
satisfied by decrypting the relevant data from this backing
stream. The encrypted stream itself does not store any data at all ---
all data is stored on its target stream.

The Encrypted Stream may contain any data at all, including disk
images, network captures or memory images. It is useful however, to
store an entire AFF4 volume within the Encrypted stream. This provides
block level encryption for the contained AFF4 volume (which might
contain arbitrary streams). It is recommended that Encrypted Streams
containing an AFF4 volume set an attribute
\emph{content-type=application/x-aff-volume} on the container stream,
to allow implementations to automatically recognize volumes. This
approach is illustrated in Figure~\ref{crypted_fif}.

\begin{figure}[tb]
  \begin{center}
  \mbox{\epsfxsize=0.8\columnwidth \epsffile{crypted.eps}}

  \caption{Embedding an encrypted AFF4 volume within an Encrypted
  Stream. The container volume contains an encrypted stream backed by
  an image stream which is also stored in the container. Once the
  encrypted stream is opened, the volume stored on its image stream is
  accessible. Now it is possible to see the secret image stream stored
  within the volume.}

  \label{crypted_fif}
  \end{center}
\end{figure}

The result is that a number of AFF4 volumes are used as {\em Container
Volumes} to provide storage for Encrypted Streams. The main {\em
Embedded Volume}, which actually contains data is stored within the
Encrypted Stream, effectively distributed throughout the container
volumes. Note that the outer Volume may contain several Encrypted
Streams and therefore contain multiple AFF4 Encrypted 
Volumes. Container Volumes may contain non encrypted streams as well,
and may implement different encryption schemes and keys for each
Encrypted scheme. This effectively allows arbitrary access policies to
be implemented as only volumes which can be accessed can be merged.

\subsection{Encryption Schemes}
Our implementation provides a number of encryption schemes.

\subsubsection{Null Encryption}
This scheme ({\em scheme=null}) specifies no encryption at all. This
may be useful for testing an implementation, but obviously provides
no real security.

\subsubsection{AES-SHA-PSK}
This scheme ({\em scheme=aes-sha-psk}) uses AES for block level
encryption, with the following SHA based ESSIV scheme, and Pre-Shared
Key (passphrase).

When creating a new encryption stream, a master key is generated using
the first 128 bits of a SHA1 hash of the Pre-Shared-Key appended to a
64 bit random salt:
\begin{eqnarray}
Key_{Master} = \left | SHA1(salt + PSK) \right | _{128}
\end{eqnarray}

For each chunk, the chunk IV is obtained by taking the first 128 bits
of the SHA1 hash of the chunk id encoded as a 32 bit little endian
integer, followed by the master key:
\begin{eqnarray}
IV = \left | SHA1(chunk\_id + Key_{Master}) \right | _{128}
\end{eqnarray}

The block is then encrypted or decrypted using AES in CBC mode with
this IV.

The salt is specified as a stream attribute named {\em aff4:salt} in
base64 encoding. Implementations are free to specify any API for
passing the Pre-Shared Key, for example the PSK can be specified in a
configuration file, typed in at the terminal, passed in using process
environment variables, or encoded within the AFF4 URL itself.

\subsection{Private Streams}
Often forensic software needs to store internal state, such as the
current state of the GUI, or internal data structures which may be
required for caching. 

Currently, software create additional cache files to maintain this
information. It is advantageous to be able to store these in the
evidence volume itself. The evidence volume can then be re-opened by
the software at a later stage and private data can be retrieved and
used directly. Such private application data can be stored in a
Private stream. There is no specification of what can be stored in the
private stream, and applications with private streams should be able
to store arbitrary segments in any format at all. Segments URNs should
be prefixed by the private stream URNs, and the stream type should be
specified as ``private''.

It is recommended that applications create attributes for the private
stream so they can reliably recognize their own private stream, and
identify the version of the application which created it. The AFFLIB
API can be used to render access to private segments and private
attributes.

\section{Usage Scenarios}
In this section we describe how AFF4 may be used in various
situations. Many of those can be solved using other forensic file
formats, but often in a more awkward way.

\subsection{Rapidly converting a set of DD images}
Many hardware devices are available to acquire hard disks in the
fields. These often produce a set of uncompressed images split at a
certain size. It is possible to construct a Map Stream which
seamlessly reassembles the logical image from all the individual disk
images. The map stream may be kept in its own volume, or appended to
one (or all) of the image fragments.

Similarly, each component can be compressed independently into its own
stream. A single map stream can then be produced to combine all the
component streams into a single logical stream. This approach can take
advantage of multiple systems to actually do the compression as each
component is compressed independently.

\subsection{Acquisition of RAID disks}
Often disks in a system are grouped into RAID devices, commonly RAID-5
or RAID-0. Previously, if disks were acquired independently, they
would need to be analysed using a tool which was able to reassemble
RAID devices.

With the AFF4 format, each of the disks can be acquired as a separate
Image Stream. Finally a tool such as PyFlag \cite{pyflag_raid} may be
used to deduce the RAID map, which can be appended to the AFF4 file as
a Map Stream. This Map Stream can then be opened by any tool to get a
logical view of the RAID, without the tool needing to have explicit
support for RAID reassembling. This approach enables parallel
acquisition of RAID drives, a feature long desired to handle the vast
quantities of data presented by RAID.

\subsection{In-place encryption of AFF4 volumes}
With the large size of a typical forensic image it is often difficult
to have to create a second copy of a volume which needs to be
encrypted. It is usually more convenient to encrypt the file in place,
overwriting the plain text data with the equivalent encrypted data.

This can be achieved using the technique shown in
Figure~\ref{inplace_crypto}. The first step is to encrypt the first
block from the front of the file and add it to the end of the archive
as a new encrypted segment. Then a ZipFile header is written to the
front which encapsulates the rest of the plain text volume. The plain
text volume is then encrypted in place. Finally a map stream is used
to re-map the first and second fragments into place.

\begin{figure}[tb]
  \begin{center}
  \mbox{\epsfxsize=0.8\columnwidth \epsffile{inplace.eps}}

  \caption{The process of encrypting a volume in place. The first
  block from the volume is encrypted and appended to the end of the
  volume as a new segment (A). Now a zip file header can be written in
  its place to form the remainder of the volume as segment B. The rest
  of the volume is encrypted in-place, block by block (Step 4). Now we
  create a Map stream which combines segment A and B into a single
  logical stream (M). An Encrypted stream which is backed by this map
  stream (E) is now added. Finally a Central Directory is appended to
  the end of the new volume. We end up with a new Zip File which
  contains our old volume stored within an encrypted stream.}

  \label{inplace_crypto}
  \end{center}
\end{figure}


\subsection{Cryptographic management of evidence}
An AFF4 archive may hold multiple encrypted volumes, each in its own
Encrypted Stream. Each of those streams is encrypted using a different
master key, and therefore can have different passphrases, and can be
assigned to different users by encrypting the master key with
different X509 certificates. Its is also possible for users to create
non-encrypted volumes within the FIF file.

This can be used to enforce access controls in line with current
legislative requirements. For example, within the same investigation
different material is often obtained under different warrants
(e.g. wiretap authorizations are different from search
warrants). Therefore, different investigators and analysts need
different access to the different streams. However, the analysts may
still store the results of their analysis in an unencrypted form, or
assign others permissions to decrypt their analysis results, without
providing access to the underlying data. 

This can be used in sharing meta data (e.g. Map Streams of files of
interest) between analysts, without needing to provide access to the
underlying data.

\subsection{Forensic Application State}
Often forensic applications need to store files other than the
evidence itself - for example, they might need to store internal data,
annotations, keyword hits etc. Currently these applications store the
data in a proprietary external file or database. This makes it
difficult to archive because the evidence itself may become separated
from the case file.

It is advantageous for these applications to store their state within
the evidence file itself using a Private Stream. Then when opening the
evidence file again, the results of their analysis will become
available.

\subsection{Sharing evidence}
When managing a large corpus of evidence it is common to require these
to be shared across the network. Many laboratories create a
centralized Network Attached Storage (NAS) facility to retain their
evidence corpus. This is typically shared via some file sharing
protocol, commonly Windows sharing (CIFS) or NFS.

It is advantageous to be able to move evidence around between several
servers and locations. For example, consider a disk image stored in a
Volume with a given URN. Normally, when a user wishes to open the
image, they will specify the path and filename of the volume which
contains it. The library will then open the volume and retrieve the
stream.

The AFF4 model is different however. In AFF4 metadata is managed
through a Universal Resolver (perhaps implemented in a database) and
does not really need to exist within a volume. This means that in the
above example, the user will query the resolver about the location of
the volume. The resolver returns a URL to the current location of the
volume. The user's library will then open a HTTP Stream instance to
the location returned and the volume may be opened directly over the
intra-web connection.

If the volume needs to be moved, the volume's \emph{aff4:stored}
attribute can be adjusted to the new location, and all clients will
automatically know where to find it. Furthermore, if several copies of
the same volume exist, the resolver may have multiple values for the
\emph{aff4:stored} property, some locations may be faster to access than
others. The Resolver can then return the most suitable attribute based
on locality.

Another interesting side effect of the sharing model is that it is far
more efficient to allow remote analysts to peruse subsets of the
evidence. For example, Alice is in the laboratory and wants Bob who is
on a remote site to view several files she found on her
image. Traditionally she would either need to copy the entire volume
and send it to him. Or maybe extract the files she thinks are relevant
and send those to him.

With the new AFF4 architecture, Bob can open the volume directly,
using the HTTP protocol, and view the files he needs. Very little
extra data is actually transferred in excess of the data he wishes to
see. This is a much more efficient way to allow remote analysts to
collaborate.

\section{Conclusion and Future Work}

This paper describes a significant enhancement to Garfinkel's Advanced
Forensic Format (AFF1). The new version, AFF4, offers significant new
features including the ability to store multiple kinds of evidence
from multiple devices in a single archive, and an improved separation
between the underlying storage mechanism and forensic software that
makes use of evidence stored using AFF. This improved system will
allow a single archive of evidence to be used in a plethora of
modalities, including in a single evidence file, multiple evidence
files stored on multiple workstations, and evidence stored in a
relational database or object management system---all without making
changes to forensic software.

We have an initial implementation of AFF4 working in the lab and plan
to integrate it into Garfinkel's. After integration and testing, we
plan to release this system as freely usable open source software.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,paper}
\end{document}



% BLS - The evidence set as identified by a particular UUID is no longer really in play 
% as below. Evidence can be grouped into a case by defining a case object and stating
% what objects (or volumes) are a part of it. Or it could be grouped into someting more 
% arbitraty depending on what you want - its all just objects and relationships.

%\item \emph{An Evidence Set UUID} is a unique identifier generated
%  according to RFC4122\cite{RFC4122} for each new Evidence
%  Set. Multiple Evidence Volumes that make up the same Evidence Set
%  should all have the same UUID. When an Evidence Set is opened all its volumes are
%  loaded, their UUIDs are checked, and all of their contents are
%  logically merged so as to be visible as if they were all found in
% the same volume.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


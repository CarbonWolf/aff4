%\documentclass[10pt,twocolumn]{article}
\documentclass[10pt, conference]{IEEEtran}
%\IEEEoverridecommandlockouts
%\documentclass{article}
%\documentclass[12pt]{IEEEtran}
%\documentclass{elsart5p}
\usepackage{listings}
\lstset{language=Python,
	frame=single,
	captionpos=b,
	frameround=tttt,
	basicstyle=\scriptsize,
	keywordstyle=\color{black}\bfseries,
                            % underlined bold black keywords
	identifierstyle=,           % nothing happens
	commentstyle=\color{white}, % white comments
	stringstyle=\ttfamily,      % typewriter type for strings
	showstringspaces=false}     % no special string spaces


% DFRWS Call Information:
% http://www.dfrws.org/2009/cfp.shtml
% Important Dates
% Submission deadline: March 16, 2009 (any time zone)
% Author notification: April 28, 2009
% Final draft due: May 19, 2009
% Pre-conference Workshops: August 16, 2009
% Conference dates: August 17-19, 2009

% Publication Criteria
% Research papers must be original contributions, not substantially
% duplicate previous work, and must not be under simultaneous
% publication review elsewhere. The review process will be
% ``double-blind'' (the reviewers will not know who the authors are, and
% the authors will not know who the reviewers are). Therefore, the
% version submitted for review should not contain the names or
% affiliations of the authors. When referring to their own previous
% work, authors should use the third person instead of the first person
% (i.e. ``Smith and Jones [2] previously determined...'' instead of ``We
% [2] previously determined..''). Authors are expected to present their
% work in person at the workshop and must have at least one registration
% per paper in order to be included in the proceedings.


% Make sure we know its a draft for now
\usepackage{draftwatermark}
\SetWatermarkFontSize{3cm}
\usepackage{fancyvrb}
%\documentclass[12pt]{article}
\usepackage{epsfig}
\usepackage[hypertexnames=false,bookmarksopenlevel=1,bookmarksopen,bookmarksnumbered,colorlinks,plainpages=false,linktocpage,linkcolor=black,citecolor=black,filecolor=black,urlcolor=black]{hyperref}
%\usepackage{natbib}
%\pagestyle{empty}
%\psdraft
%\baselineskip=20pt %Sets line spacing to 1 unit
%\bibliographystyle{elsart-num-names}
\bibliographystyle{IEEEtran}
\usepackage{cite}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1:}]}{\end{trivlist}}

%\begin{frontmatter}
\begin{document}
\title{Extending the Advanced Forensic Format to accommodate Multiple
  Data Sources, Logical Evidence and Forensic Workflow}
\author{Blinded for review}
%\author{M. I. Cohen\footnote{scudette@gmail.com}, Simson Garfinkel, and Bradley Schatz}
%\author{M. I. Cohen}
%\address{M. I. Cohen is a Data specialist with the Australian Federal Police, Brisbane, Australia}
%\baselineskip=20pt %Sets line spacing to 1 unit
%\end{frontmatter}
%\thanks{M. I. Cohen is a Data specialist with the Australian Federal Police, Brisbane, Australia}
\maketitle

\begin{abstract}
Forensics analysis requires the acquisition and management of many
different types of evidence material, including individual disk
drives, RAID sets, network packets, memory images, and extracted files. Often the same evidence is
reviewed by several different tools or examiners in different
locations. We propose a backwards-compatible evolutionary redesign of the Advanced
Forensic Format an open, extensible
file format for storing of evidence and interchanging analysis results
among different tools. The Forensic Interchange Format (FIF) was
designed to be simple to implement, relying on the well supported Zip
specifications for bit level file access.
\end{abstract}

\section{Introduction}

Unlike other kinds of evidence, digital evidence can be instantly lost
or corrupted if technical measures are not immediately taken to
preserve its existence and integrity. As a result, preserving digital
evidence is an important part of most digital
investigations\cite{carrier:event-based}.

In recent years there has been a steady and growing interest in the
actual file formats and containers used to store this digital
evidence. Early practitioners created exact bit-for-bit copies (``mirror disks'') employed
proprietary software systems (e.g. \cite{safeback,ilook,encase}) for
making and authenticating ``images'' of digital evidence. PyFlag\cite{pyflag}
introduced a ``seekable gzip'' format that allowed disk images to be
stored in a form that was compressed but allowed the random-access to
evidence data necessary for forensic analysis. The Advanced Forensic
Format (AFF) expanded on this idea with a forensic file format that
allowed both data and arbitrary metadata to be stored in a single
digital archive\cite{garfinkel:aff}. Recently the open source AFF
implementation library (AFFLIB) was expanded to support both
encryption and digital signatures of digital evidence\cite{garfinkel:affcrypto}.

This paper extends the previous work by adapting AFF to the multiple
heterogeneous data types that might arise in a modern digital
investigation, including data from multiple data storage devices, new
data types (including network packets and memory images), extracted
logical evidence, and forensic workflow. We do this in a manner that
is at once upwards compatible with AFF while adopting the AFF
bit-level specification so that AFF files can be processed using
standard non-forensic tools.  We call the new system AFF4, and use the
phrase AFF1 to refer to the legacy system developed by
Garfinkel.\footnote{Although Garfinkel never changed the AFF bit-level
specification, Garfinkel released AFFLIB implementations with major
version numbers 1, 2 and 3. We therefore call our system AFF4 to avoid confusion.}

\begin{figure}
\fbox{
\begin{minipage}{.95\linewidth}
\begin{itemize}
\item The ability to store evidence from one or more physical
  devices.
\item Schema allowing evidence files to contain memory
  images, network packet intercepts, Netflow data, and extracted
  files.

%% Raid is only one example of the application of the map stream - the
%% most important might be the ability to use sleuthkit to generate
%% block allocation tables for all the files - thereby allowing
%% arbitrary programs to read individual files without using sleuthkit
\item Direct encoding of arbitrary mapping transformations of one or
more sources to allow multiple logical views. For example, allowing
RAID volumes to be automatically reassembled from evidence files made
of the physical RAID devices.
\item A revised compression system that allows individual chunks
  within a 16MB segment to be decompressed without decompressing the
  entire segment.
\item Support for Global Distributed Evidence Management.
\end{itemize}
\end{minipage}}
\caption{Specific design goals for the next generation forensic file
  format.}
\end{figure}
\section{The Need for an Improved Forensic Format}

AFF1's flexibility came from the fact that forensic data and metadata
is stored as arbitrary name/value pairs called \emph{segments}. For
example, the first 16MB of a disk image is stored in a segment called
\texttt{page0}, the second 16MB in a segment called \texttt{page1},
\emph{etc.} Because of this flexibility, it was relatively easy for
Garfinkel to extend AFF1 to support encryption, digital signatures, and
the storage of new kinds of metadata such as chain-of-custody
information\cite{garfinkel:affcrypto}. 

\subsection{AFF Limitations}
After imaging literally thousands of devices using Garfinkel's AFF1
tools and adding support for AFF1 to numerous open source programs, we
observed numerous problems in the underlying standard
and Garfinkel's AFFLIB implementation:

\begin{itemize}
\item While AFF1's design stores a single disk image in each evidence
  file, modern digital investigations typically involve many seized
  computers or pieces of media. 
\item AFF1 has no provision for storing memory images or intercepted
  network packets.
\item AFF1 has no provisions for storing extracted files that is
  analogous to the EnCase ``Logical Evidence File'' (L01) format, or
  for linking evidence to web pages.
\item AFF1's encryption system leaks information about the contents of
  an evidence file because segment names are not encrypted.
\item AFF1's default compression page size of 16MB can impose significant overhead
  when accessing NTFS Master File Tables (MFT), as these structures
  tend to be highly fragmented on systems that have seen significant
  use. 
\item Although the AFF1 specification calls for a ``table of contents'' similar
  to the ZIP\cite{zip-format} ``central directory'' that is stored at the end of AFF
  files, Garfinkel never implemented this directory in the publicly
  released AFF1 implementation, AFFLIB. As a result, every header of every segment in an
  AFF file needs to be read when a file is opened. It practice this
  can take 10--30 seconds the first time a large AFF file is
  opened. (Once the file is opened, the sectors corresponding to the
  segment headers are stored in the computer's disk cache.)
\item AFF1's bit-level specification is essentially a simple container
  file specification. Given the that there are other container file
  specifications that are much more widely supported with both
  developer and end-user tools, it seemed reasonable to migrate AFF
  from its home-grown format to one of the existing standards. 
\end{itemize}

\section{Global Distributed Evidence Management}
While Garfinkel designed AFF for use on a single machine that could
both image evidence and perform analysis, the main use of AFF in
practice has been in distributed environments in which imaging and
analysis takes place in multiple locations and is performed by
multiple individuals.

Managing evidence in a globally distributed system requires the use of
globally unique identifiers. The original AFF specification assigned
each piece of evidence a unique 128-bit identifier called a GID but did
not make it clear when this identifier should be changed and when it
should remain the same. 

In practice, however, global distributed evidence management requires
more than simply tracking the movement of disk images: it requires
approaches for sharing evidence to multiple disconnected
evidence, allowing offline work, and then seamlessly recombining the
work products of the analysts in a third security domain.

Consider the typical usage scenario depicted in Figure \ref{usage}, of
an file which contains a disk image. This volume is distributed to two
independent analysts, Alice and Bob. Alice may find and extract
individual files, while Bob may correlate information in the evidence
file with other data that is available on departmental
servers. Although in some environments Alice and Bob may be able to
work on a shared file that is located on a server, in other
environments there will not be sufficient bandwidth or
connectivity. Instead, each analyst will be required to store the
information in their own evidence file; these files will then be
recombined at a later point in time.

In this case they can
each create a new volume which extends to original volume and save
their analysis on this new volume. Now they only need to share this
new volume with other analysts who also have a copy of the old volume
to interchange their findings. 

This is made possible because each volume is independent of one
another, but can still be recombined into a bigger evidence set. 

\begin{figure}[tb]
  \begin{center}
  \mbox{\epsfxsize=0.6\columnwidth \epsffile{usage1.eps}}
  \caption{A typical usage scenario. Both Alice and Bob receive an AFF
  volume but work independently. Rather than modifying the volume,
  they each create their own local volumes and save their results into
  those files. They can now exchange the smaller new volumes and
  effectively merge their results into the same AFF set when they are finished.}
  \label{usage}
  \end{center}
\end{figure}

\subsection{Migration from AFF1 to AFF4}

We have addressed these issues in three steps. First, we have extended
the AFF schema to support multiple forensic images (which are now
called \emph{streams}) in a single evidence file. Second, we replaced
AFF's ``bit-level'' format with an improved format that is more
flexible and has better existing support. Third, we are adopting
AFFLIB to implement the new schema and file format, which will allow
existing AFF-aware programs to access files written in either the old
or the new format without modification.

We call the result of our work effort AFF4.

\section{Introducing AFF4}

This section discusses the AFF4 terminology. 

%% Since this is the itemised definition its probably better to
%% restrict it to the basic definitions and then move most of the
%% discussion a bit later. I want to show how concepts build on one
%% another - maybe even put a UML or class diagram

\begin{itemize}
\item \emph{An AFF Object} is the basic building block of our
file format. AFF Objects have a globally unique name (URN) as described in
\cite{RFC1737}. The name is defined within the aff2 namespace, and is
made unique by use of a unique identifier generated as per
RFC4122\cite{rfc4122}.

\item \emph{A Relation} is a statement about an AFF Object which
expresses some property of the object. The relation comprises of a
triple of (Subject, Attribute, Value). All metadata is reduced to the
triple notation.

\item \emph{An Evidence Volume} is a type of AFF Object which is
responsible for providing storage to AFF segments. Volumes must
provide a mechanism for storing and retrieving segments by their
URN. We discuss two volume implementations below, namely the {\em
Zip64 based volume} and the {\em Directory} based volume.

\item \emph{A segment} is a single unit of data written to a volume. AFF4
  segments have a \emph{segment name} provided by their URN, a
  \emph{segment timestamp} in GMT, and the \emph{segment contents}.

\item \emph{A stream} is an AFF Object which provides the ability to
seek and read random data. Stream objects implement abstracted
storage, but must provide clients the stream like interface. For
example, we discuss the {\em Image stream} used to store large images,
the {\em Map stream} used to create transformations and the {\em
Encrypted stream} used to provide encryption.

% Is it important to define this? do we use it?
%\item \emph{An Evidence Set} is a loose collection of one or more Evidence
%Volumes.

%Evidence Sets can also be stored in object storage system (such as
%  Amazon S3), or a hybrid---for example, metadata stored in an SQL
%  database with physical data stored in large files in a file
%  system. 

% I would describe this process after the definitions.

%\item \emph{An Evidence Set UUID} is a unique identifier generated
%  according to RFC4122\cite{rfc4122} for each new Evidence
%  Set. Multiple Evidence Volumes that make up the same Evidence Set
%  should all have the same UUID. When an Evidence Set is opened all its volumes are
%  loaded, their UUIDs are checked, and all of their contents are
%  logically merged so as to be visible as if they were all found in
% the same volume.

%% I Wonder if this explaination should be postposed to the section we
%% actually discuss it - otherwise we duplicate it again
%% \item \emph{An image stream} is a stream that is dedicated to storing evidence from a single
%%  source of data.  For example, a stream may contain a disk image or a network
%%  capture. Whereas AFF could only support a single stream in an evidence
%%  file, AFF4 allows any number of streams to be stored. Each stream in
%%  the archive is  given a unique identifier.

%%\item \emph{A bevy} is a specific segment that is used to hold
%%  forensic image data, such as disk sectors, memory pages, or network
%%  packets. AFF1 used the term ``pages'' to describe bevys, but this
%%  term caused confusion when AFF was used to hold memory images, hence the
%%  new term. 


%% This is no longer correct - segment names are URNs and are encoded
%% according to RFC1737 as mentioned above.
%%\item \emph{A segment name} is the name under which a segment is
%%  stored within the archive. AFF4 segments have two-part names that
%%  typically contain a \emph{stream name} and \emph{bevy number}. For
%%  example the segment named \texttt{default/000001} refers to the stream
%%  called \texttt{default} and has a chunk name of \texttt{000001}. All segment
%%  names are encoded using UTF8. AFFLIB is being modified to allow any
%%  stream to be opened using the \texttt{af\_open} function; if a
%%  stream name is not specified, AFF4 implementations will open the
%%  stream named ``default.''

%%\item \emph{The segment timestamp} is a timestamp for the segment
%%  recorded in GMT.


%% This should be discussed as a way we are storing the RDF triples,
%% rather than as a definition.
%% \item \emph{Attributes} are a list of key/value pairs stored in each
%% stream's ``properties'' segment (e.g. ``stream/properties''). The
%% ``properties'' file is a text file which stores attributes one on each
%%line. Attribute lines consist of the key, immediately followed by
%%``='' for text values or ``:'' for binary values. This is immediately
%%followed by the encoded value. Values containing text are encoded
%%using UTF8, while values containing binary data are encoded using
%%Base64. There may be multiple values for each attribute; in these
%%cases the order is preserved
%%by implementations. Implements may present different keys may be
%%presented in any order, however.
%%\vspace{1ex}

%% This is not correct any more as volumes must store their own URN in
%% a different way (the zip implementation uses an archive comment for
%% this).

%% Each AFF4 archive must also have a top-level set of attributes---that
%% is, there is always a segment named ``properties.'' These volume
%% attributes are used to specify a Unique Identifier (UUID). (AFF1
%% stored each property in its own named segment. This created
%% significant overhead in many situations.)

%% Since target references are all just URNs now the resolving is automatic.

\item \emph{A Target Reference} is a way of referencing objects by use
of a Uniform Resource Identifier (URI). The URI can be another AFF
Object URN or may be a more general Uniform Resource Locator (URL),
such as for example a HTTP or FTP object.

\item \emph{The Resolver} is a central data store which collects and resolves
attributes for the different AFF Objects. We discuss the Resolver in
detail in Section \ref{resolver}.

%%in a number of places which will be highlighted below. Using
%%references it is possible for the FIF file to refer to objects
%%embedded within itself, or within other FIF files. Its is also
%%possible to refer to external flat files. A number of usage scenarios
%%are described in Section \ref{usage_scenarios}. 
\end{itemize}

Since AFF Objects have globally unique URNs it is possible for an
Object in one volume to refer to an object in another volume. The
resolver has visibility of all URNs and their current physical
location.

%The Target Reference is an important AFF4 innovation: this mechanism
%gives AFF4 files a way for describing evidence that is stored in
%different evidence volumes, in the host computer's file system, or on
%the Internet. We have defined the following target reference schema:

\label{target_reference}

%\begin{itemize}
%\item A reference in the format {\em aff://UUID/stream/segment} refers
%to an external Evidence file set (identified by its UUID). If the segment
%is missing (i.e. the reference ends with a ``/''), the reference is
%for the entire stream. We do not specify a way of actually resolving
%the UUID into a physical locations. This can achieved by
%implementations using many methods, such as Active Directory, LDAP,
%DNS, flat files or SQL databases.

%\item A reference starting with {\em file://} refers to an external
%file. The external file reference must be sanitised (to remove
%.. directory traversal) and rooted at the current directory. A
%filename of ``.'' refers to the current FIF volume.

%\item A reference starting with {\em http://} refers to a HTTP
%object which will be fetched. Support for this is optional.

%% This is no longer correct as all segments, streams and volumes have
%% a unique URN so we dont need to make these distinctions.

%% \item A references ending with a ``/'' signifies a reference to a
%% stream in the current FIF file. The stream will be opened in its
%% entirety. If the reference does not end with a ``/'', the reference is
%% for a single segment.
%%\end{itemize}

\section{Metadata and the Universal Resolver}
The AFF4 model treats metadata as an abstract concept which may exists
independently from the data itself. We term {\em metadata} to be a set
of statements about objects, written in triple notations (Subject,
Attribute, Value), where {\em Subject} is the URN of the object the
statement is made about. Using this system we are able to store
arbitrary attributes about any object in the AFF4 universe.

The AFF4 design extends beyond the management of a single volume,
stream or image to a universal system for managing data of many
types. This necessarily means that a single running instance is
generally unable to have visibility of the entire AFF4 universe. For
example, if a volume is opened which contains a Map Stream targeting a
stream stored in a different volume, its not generally possible to
tell in which volume the stream exists, and where that volume is
actually stored.

To provide this global visibility of metadata we define a central
metadata management entity, named the {\em Universal Resolver}. The
Universal Resolver contains all the metadata about the AFF4 universe,
that is to say it is able to resolve queries about any attribute about
any URN in the universe.

Although the resolver has complete visibility of all attributes, it is
still useful to store metadata within the volume itself, particularly
data pertaining to the volume itself. If we did not store the metadata
within the volume itself, then the volume would not be accessible to
implementations which do not have this metadata.

To this end we define a way for serializing metadata statements (or
triples) into a standard format which implementations can load into
their respective resolvers when parsing the volume. Triples can be
stored in segments having a URN ending with ``{\em properties}''. The
AFFLIB implementation loads these segments automatically into the
Universal Resolver. 

Triples are stored within the properties segment one per line, with
the subject URN (encoded according to RFC1737), followed by whitespace
and the attribute name. This is then followed by the equal sign and
the UTF8 encoding of the value. An example properties file for an
Image Stream is shown in Listing \ref{image_metadata}.

It is important to stress that the properties file is simply a
serialization of statements into volume segments. The statements may
exist without being stored in a volume (for example, being stored on
an external SQL server). Alternatively, these statements may be stored
in some other way inside or outside the volume (e.g. SQLite
databases).

When the volume is loaded, the AFFLIB implementation automatically
loads any properties files and populates its Universal Resolver with
the information it has visibility of. AFFLIB provides a mechanism to
use an external resolver stored on a MySQL database to provide a
Universal Resolver that shares information between different instances
on the same network.

Each URN within the AFF4 universe must have an ``\texttt{type}''
attribute to denote the type of the Object.

\begin{lstlisting}[
	caption=Example properties files for several AFF4 objects (URNs are
shortened for illustration).,
	label=image_metadata,
	float=tb,
	 ]
Directory Volume:
  urn:aff2:f901be8e-d4b2 aff2:stored=http://../case1/
  urn:aff2:f901be8e-d4b2 aff2:type=directory

ZipFile Volume:
  urn:aff2:98a6dad6-4918 aff2:stored=file:///file.zip
  urn:aff2:98a6dad6-4918 aff2:type=zip

Image Stream:
  urn:aff2:83a3d6db-85d5 aff2:stored=urn:aff2:f901be8e-d4b2
  urn:aff2:83a3d6db-85d5 aff2:chunks_in_segment=256
  urn:aff2:83a3d6db-85d5 aff2:chunk_size=32k
  urn:aff2:83a3d6db-85d5 aff2:type=image
  urn:aff2:83a3d6db-85d5 aff2:size=5242880

Map Stream:
  urn:aff2:ed8f1e7a-94aa aff2:target_period=3
  urn:aff2:ed8f1e7a-94aa aff2:image_period=6
  urn:aff2:ed8f1e7a-94aa aff2:blocksize=64k
  urn:aff2:ed8f1e7a-94aa aff2:stored=urn:aff2:83a3d6db-85d5
  urn:aff2:ed8f1e7a-94aa aff2:type=map
  urn:aff2:ed8f1e7a-94aa aff2:size=0xA00000

Link Object:
  map aff2:target=urn:aff2:ed8f1e7a-94aa
  map aff2:type=link
\end{lstlisting}

\section{Volumes}
The volume object is responsible for providing storage for
segments. Segments are stored and retrieved using their URNs. We
describe two different implementations of volume objects, namely the
{\em Directory Volume} and the {\em ZipFile Volume}. It is possible to
convert from one implementation to another easily, without effecting
any external references. This is because external references are to
the Volume URN itself, so its safe to convert from one representation
to another. In this way its possible for example, to unzip a Zip64
volume into a directory and vice versa.

\subsection{Directory Volumes}
The Directory Volume is the simplest type of volume. It simply stores
different segments based on their URNs in a single directory. Since
some filesystems are unable to represent URNs accurately (e.g. Windows
has many limitations on the types of characters allowed for a
filename), the Directory Volume encodes URNs according to RFC1738
\cite{RFC1738}; non-printable characters are escaped with a \%
followed by the ASCII ordinal of the character.

The Directory Volume uses the {\em aff2:stored} attribute to provide a
base URL. The URL for each segment is then constructed by appending
the escaped segment URN to the base URL. Note that there is no
restriction on what type of URL this can be, so it may be a location
on a filesystem (e.g. {\em file://some\_directory/}) or a location on a
HTTP server (e.g. {\em http://intranet.server/some\_path}). In this
way its possible to move the entire volume from a filesystem to a web
server transparently.

The Directory Volume stores its own URN in a special segment named
``{\em \_\_URN\_\_}'' at the base of the directory.

\subsection{ZIP64}
For AFF4, we have changed the default volume container file format to
ZIP64\cite{zipspecs}. There are many reasons for this decision:

\begin{itemize}
\item There is already wide support for the ZIP and ZIP64 formats. By
  migrating to these formats, we can take advantage of the rich number
  of user and developer tools already available. The volume may be
inspected using any number of commercial zip application (e.g. Windows
Explorer natively supports Zip files).
 
%% In not sure how this is relevant
%\item ZIP64 includes a CRC32 and timestamp for each segment. We
%  believe that the additional integrity checks and timestamps will be
%  useful in a forensic environment.

\item ZIP64 already includes support for the ``table of contents''
  envisioned but never implemented by Garfinkel; ZIP64 calls this
  table the ``Central Directory.'' It is written at the end of the
  ZIP64 volume.

\item ZIP64 libraries are readily available making proprietary implementations of
interfaces to the AFF4 volume format simple to write. For example, a
simple python program to dump out an Image stream is illustrated in
Listing \ref{python_code}.

\end{itemize}

\begin{lstlisting}[
	float=tb, label=python_code, caption=Sample Python code to
	dump out an Image Stream. As can be seen the chunk index
	segment is used to slice the data segment into chunks. The
	chunks are decompressed and written to the output file.]
volume=zipfile.ZipFile(INPUT_FILE)
outfd = open(OUTPUT_FILE,"w")
count = 0

while 1:
    idx_segment = volume.read(STREAM+"/%08d.idx" % count)
    bevy = volume.read(STREAM+"/%08d" % count)
    indexes = struct.unpack("<" + "L" * 
	(len(idx_segment)/4), idx_segment)

    for i in range(len(indexes)-1):
	chunk = bevy[indexes[i]:indexes[i+1]]
        outfd.write(zlib.decompress(chunk))

    count += 1
\end{lstlisting}


Figure~\ref{zip_structure} shows the basic structure of a Zip
archive. As can be seen, the archive consists of a {\em Central Directory} (CD)
located at the end of the archive. The CD is a list of pointers to
individual {\em File header} structures located within the body of the
archive. Headers are then followed by the file data, after it has been
compressed by the appropriate compression method (as specified in the
header). Each archived file is optionally followed by a {\em Data
Descriptor} describing the length and CRC of the archived file. Using
the data descriptor field allows implementations to write archives
without needing to seek in the output file. This allows Zip files to
be written to pipes for example, sending an image over the network
using netcat or ssh.

\begin{figure}[]
  \begin{center}
  \mbox{\epsfxsize=0.8\columnwidth \epsffile{zip_structure.eps}}
  \caption{The basic structure of a Zip archive}
  \label{zip_structure}
  \end{center}
\end{figure}

It is important to note that AFF4 only requires that the volume be
capable of storing multiple named segments of data. Although our AFF4
implementation uses the ZIP64 file format as an underlying storage
mechanism, our system also supports legacy AFF1 volumes as well as
segments that are stored in other storage systems---for example,
Amazon.com's Simple Storage System (S3)\cite{s2-aws-home-page-money}.

\subsection{Unused ZIP64 Features}
There are a number of aspects in the Zip file format specifications
which we do not utilize.
\begin{itemize}
\item We ignore Zip64's built-in support for splitting
archives into multiple ZIP files. Instead, our implementation treats
each volume as complete and stand-alone ZIP file. The AFF4
implementation then considers the segments contained within as
belonging to the universal collection. We provide the ability to split
a stream across volumes automatically, as each segment may be stored
in a different volume, and the Universal Resolver is used to find it.

\item Although Zip64 also defines encryption and authentication
extensions, we do not use them due to the restrictions imposed on
their use and because they lack of functionality that is important for
forensic user. Instead, we use AFF1's digital signature facilities for
integrity and non-repudiation, and we introduce a new stream based
encryption scheme for ensuring data privacy
(Section~\ref{crypted_stream}).

\end{itemize}

%% Im not sure this is really that relevant here. Byte range updates
%% can be achieved by an append and a map operation, which might be a
%% better option for aimager working in dd_rescue mode.

%\subsubsection{A Forensically Sound Strategy For Updating ZIP64 Files}
%In many forensic applications, individual segments within the volume
%need to be updated. To this end, Garfinkel implemented several
%``update'' functions in AFF1\cite{garfinkel:aff}. 

%There were two problems with the original AFF updating approach:
%\begin{itemize}
%\item Updates can cause the volume to
%become fragmented as holes appear within the middle of the volume, and
%updated segments are added at the end.
%\item Updates could be performed silently, raising questions of
%  forensic soundness.
%\end{itemize}

%In AFF4, when a new segment is added to an existing volume, the new
%segment header overwrites the volume central directory, and the
%central directory should be rewritten including the new entry
%immediately after the last file in the archive. If it is not possible
%to overwrite the file, the new segment can be written at the end,
%followed by an updated copy of the central directory (in that case the
%old central directory remains in the file). This is illustrated in
%Figure \ref{zip_structure}. Implementations may delay
%writing the central directory until the file is closed; should the
%system crash before the central directory is written, it can be
%recovered by scanning the entire ZIP file.

%A segment within the archive is considered to be updated when a new
%segment is added to the archive with the same name and a later
%timestamp. Implementations satisfy read requests for duplicate
%segment names with those segments with the later timestamp, or if the
%two timestamps are the same, the segment which appears at a later
%offset in the volume. This effectively allows for a segment to be
%updated without leading to internal volume fragmentation.

%The fact that the older version of all updated segments remains intact
%within each volume can be used for auditing, or to allow a rollback to
%previous versions of the segment. We have created a new tool,
%\texttt{afrepack}, which can repack, split and manage AFF4 volumes by
%creating new volumes and redistributing segments among them.

\section{Streams}
The Stream object provides random access to an abstract representation
of a body of data. Our implementation allows the segments in a stream
to be operated on as if they were a single file by supporting the
traditional POSIX-like functionality of
\texttt{open()}, \texttt{seek()}, \texttt{write()}, and
\texttt{read()}. All streams must have the ``\texttt{size}'' attribute
to denote the last bytes addressible within the stream. This is
required in order to support the POSIX whence attribute which may
require seeking from the end of the stream.

The following describes a number of types of streams. It is important
to note that clients of our implementation do not care how a
particular stream is implemented. Streams are opened by their URNs,
and the library itself ensure they provide the Stream interface. So
for example, users do not care if a stream is a Map Stream or an Image
Stream - the data provided is the same.

%% Not sure how its relevant - maybe we can move it to a new section
%% about randomly writing stream.

%AFFLIB also supports the \emph{writing} of streams, a feature that is
%used by Garfinkel's \texttt{aimage} disk imaging program. In general
%arbitrary writing to streams can be expensive, especially when writing
%is combined with seek operations. The current \texttt{aimage} program
%will create AFF files with gaps in them during its error recovery
%process (when \texttt{aimage} seeks to the end of a disk and reads it
%backwards in an attempt to get around media failures). We are working
%on a rewrite of \texttt{aimage} which performs imaging and recovery
%on bevy boundaries, rather than using the current arbitrary
%``high-water-mark'' and ``low-water-mark.''

%All streams within an Evidence Set \emph{must} have a properties segment. The
%properties segment describes what the stream contains and determines
%how the stream is processed. 

\subsection{The Image Stream}
\label{image_stream}
The AFF4 \emph{Image Stream} stores a single read-only forensic data
set. For example, this stream might contains a hard disk image, a
memory image or a network capture (in PCAP format). Image streams have
an \texttt{aff2:type} attribute of \texttt{image}.

Storage for the data is done by using multiple data segments stored on
various volumes. Data segment URNs are derived by appending an 8
digit, zero padded decimal integer representation of an incrementing
id to the stream URN (e.g. ``urn:aff2:83a3d6db-85d5/00000032''). Each
data segment is called a bevy and stores a number of compressed chunks
back to back. The chunk index segment is a segment containing a list
of relative offsets to the beginning of each chunks within the
bevy. The chunk index segment URN is derived  by appending the bevy
URN with ``.idx''. This is illustrated in Figure \ref{image_stream}.

\begin{figure}[tb]
  \begin{center}
  \mbox{\epsfxsize=0.6\columnwidth \epsffile{image_segments.eps}}
  \caption{The structure of Image Stream Bevies. Each bevy is a
collection of compressed chunks stored back to back. Relative chunk
offsets are stored in the chunk index segment.}
  \label{image_stream}
  \end{center}
\end{figure}

Image streams specify the \texttt{chunk\_size} attribute, as the
number of image bytes each chunk contains (typically chunk sizes are
32kb). Also specified is the \texttt{chunks\_per\_segment} attribute
which specifies how many chunks are stored in each bevy. Each chunk is
compressed individually using the zlib compress algorithm. This
general structure of storing chunks within larger segments similar to
the technique used by the Expert Witness file format used by
EnCase\cite{encase-3.0} and implemented by the open source
libewf\cite{libewf} package. The advantages of keeping smaller sized
chunks is the better match between requested size and the minimum size
required for decompression. This means that less data is needed to be
decompressed unnecessarily. 

%% FIXME - hashing may apply to any segment not just bevies
%Bevies can optionally be individually hashed, allowing for detection
%of modifications. The attribute \texttt{hash\_type} specifies the type
%of hash used (implementations will typically support md5,sha1 and
%sha256, but can define others). The hashes are stored in any number of
%segments within the stream, of arbitrary name. The hash segment names
%is provided by using the \texttt{hash\_name} stream attribute.

%Hashes are calculated in an identical way to that defined by AFF1's
%crypto layer\cite{garfinkel:affcrypto}---that is, the full name of the
%segment (including stream component) is followed by a single NULL
%byte, followed by the (uncompressed) content of the chunk. The
%inclusion of the segment name ensure that even identical chunks in
%content will have a different hash and prevents reordering attacks.

%Each hash segment is a list of a chunk id encoded in 32 bit
%little endian integer format followed by the hash (the length of which
%depends on the hash itself). This approach is illustrated in Figure
%\ref{hash_index}. 

%Hash segments can contain any number of chunk hashes in them. It is
%recommended that a hash segment be created when a volume is closed to
%cover all the chunks within that volume. This allows for hashes to be
%verified even if some volumes are missing. The AFFLIB implementation
%may update a hash of a chunk previously set by an earlier hash
%segment---this would be required when a chunk is updated. The hash
%segment can then be signed to assure the integrity of the updated segment.

%\begin{figure}[tb]
%  \begin{center}
%  \mbox{\epsfxsize=0.6\columnwidth \epsffile{hash.eps}}
%  \caption{The structure of a hashing segment. The attributes {\em
%  hash} and {\em hash\_type} declare the hash segment. The segment
%  contains a list of integer encoded chunk id followed by an unencoded
%  hash for each chunk.}
%  \label{hash_index}
%  \end{center}
%\end{figure}

%This design differs from AFF1, which creates a separate segment for
%each hash. AFF4 uses headers that are more verbose than the AFF
%segment headers. Therefore, creating a new segment for each hash value
%would result in too much overhead: Packing the hashes into a few
%segments it is possible to reduce this overhead to a minimum.

\subsection{The Map Stream}
\label{map_stream}
Linear transformations of data are commonplace in forensic
analysis. For example, a file is often simply a collection of bytes
drawn from an image, while a TCP/IP stream is simply a collection of
payloads from selected network packets. 
Sometimes the same data may be
viewed in a number of ways---for example a Virtual Address Space is a
mapping of the Physical Address Space through a page table
transformation.  Zero Storage Carving \cite{Meijer2006} is a way of
specifying carved files in terms of a sequence of blocks taken from
the image; Cohen extended this concept to an arbitrary
mapping function\cite{Cohen2007} which can be used to describe
arbitrary mappings of carved files within a single image. 

In this work we extend the mapping function concept to allow a single
map to draw data from arbitrary streams (called {\em targets}). This
transform is implemented via the {\em Map stream}.

The mapping function is described in a segment named by appending a
``/map'' to the stream URN. The segment data consists of a series of
lines, each containing a stream offset, followed by a comma, and a
target offset. Offsets are encoded using decimal notation, and are
followed by a target URN.

Denoting the stream offset by $x$, and the target offset by $y$, the
Map specifies a set of points $(X_i,Y_i,T_i)$. Read requests for a
byte at a mapped stream offset $x$ can then be satisfied by reading a
byte from target $T_i$ at offset $y$ given by:
\begin{eqnarray}
y = (x - X_i) + Y_i & &
\forall x \in \left [X_i, X_i+1 \right )
\end{eqnarray}

For example, consider the following map:
\begin{lstlisting}
0,0,urn:aff2:83a3d6db-85d5
4096,10000,urn:aff2:f901be8e-d4b2
8192,5000,urn:aff2:83a3d6db-85d5
\end{lstlisting}

To read this stream we satisfy read requests of offsets between 0 and
4095 in the stream from offset 0 to offset 4095 in
\emph{urn:aff2:83a3d6db-85d5}. Requests for bytes between 4096 and 8191 are
fetched from \emph{urn:aff2:f901be8e-d4b2} at offset 10000. Finally
bytes after 8192 (until the specified size of the stream) are fetched
from offset 5000 in \emph{urn:aff2:83a3d6db-85d5}.

In order to efficiently express periodic maps such as those found in
RAID arrays, the Map stream may be provided with two optional
parameters a {\em target\_period} ($T_p$), and {\em stream\_period}
($S_p$). If specified, the above relation becomes:
\begin{eqnarray*}
p &:=& floor\left (\frac{x}{S_p} \right) \\
x' &:=& mod(x ,S_p)  \\   \label{eq:no1}
y &:=& (x'-X_i) + Y_i + p \times T_p
\end{eqnarray*}

Where $mod$ is the modulus function and $floor$ signifies integer
division. For example consider Listing~\ref{map}, which corresponds to a 3
disk RAID-5 array.

\begin{lstlisting}[
	float=tb, caption=A Map stream that corresponds to a 3 disk
	RAID-5 array. The targets are URNs for the respective
        disks. Note that map coordinates are given in multiples of 
	block size.,
	label=map, language=]
aff4:block_size=64k 
aff4:stream_period=6 
aff4:target_period=3

0,0,disk1
1,0,disk0
2,1,disk2
3,1,disk1
4,2,disk0
5,2,disk2
\end{lstlisting}

%\Subsection{The Overlay Stream}
%The Overlay Stream Is A Variant Of The {\Em Image} Stream That
%Provides A Layer Of Indirection Between Requests Made For Specific
%Bytes (Or Chunks Or Bevys) And The Actual Location Where The Requested
%Information Is Stored. The Main Utility Of The Overlay Stream Is In
%Providing Transparent Access To Legacy Forensic Formats, Such As Ewf.

%To Access These Legacy File Formats We Only Need To Have An Index Of
%Each Chunk Offset, And Then Directly Use The Overlayed Files.  The {\Em
%Ewf2aff} Tool Provides Such An Overlay Feature For Ewf Volumes By
%Employing Libewf To Scan The Internal Data Structures.

%Overlay Stream Require That A \Texttt{Chunk\_Size} Attribute Be Specified. In
%Addition The {\Em Compression\_Type} Attribute May Be Specified. The
%Attribute Should Contain An Integer Corresponding To A Suitable Zip
%Compression Scheme (E.G. Deflate). If Not Specified It Is Assumed
%That No Compression Is Used.

%The ``Target'' Stream Attribute May Be Specified Multiple Times And
%Refer To External Files (See Section \Ref{Target_Reference}). There
%Can Be A Number Of Overlay Segments Named ``Stream Name/Overlay.00''
%Which Are All Merged To A Single Overlay. If More Than One Overlay Is
%Present, The Attribute {\Em Overlays} Must Be Specify How Many.

%The Overlay Segment Contains A Series Of Lines With Comma Delimited,
%Decimal Encoded, 64 Bit Integers Representing Chunk Number, Target
%Offset, Target Length And Target Number. Target Number Is An Index
%Into The Targets Defined In The Attributes (Count Starts With Zero).
%Chunks Are Then Fetched From The Specified Target And Decompressed If
%Necessary.

%It Is Possible To Refer To The Current File By Providing A Target Of
%``File://.''. This Can Be Useful For Modifying An Existing Ewf File To
%Be A Aff4 File. Since Zip Files Are Normally Read From The End Of The
%File, But Ewf Are Both Read From The Front Of The File, It May Be
%Possible To Append A Aff4 Central Directory To The End Of An Ewf File
%Without Interfering With The Overlayed File. In This Case The
%Converted File Can Still Be Used As An Ewf File Without Change, But
%Could Also Be Used As An Aff4 File.

%In Image Streams Some Chunks May Compress Very Well. In That Case, The
%Overheads Introduced By The Zip File Header For Each Chunk In The
%Image Stream May Become Unacceptably High. In That Case Its Possible
%To Use An {\Em Overlay Stream} To Coalesce Chunks Into Larger
%Segments. This Approach Is Illustrated In Figure \Ref{Overlay}.

%\Begin{Figure}[Tb]
%  \Begin{Center}
%  \Mbox{\Epsfxsize=0.6\Columnwidth \Epsffile{Overlay.Eps}}
%  \Caption{The Use Of An Overlay Stream To Overlay An Image With
%Coalesced Chunks.}
%  \Label{Overlay}
%  \End{Center}
%\End{Figure}

%In The Above Figure A Segment Is Written To The Aff4 Volume Which
%Contains A Number Of Chunks Back To Back. An Overlay Stream Then Uses
%Direct References To Each Compressed Chunk Within The Coalesced
%Segment. The Target Is Then Specified As The Segment Name. Although
%This Technique Is Valid, It Is Generally Discouraged Since It Breaks
%The Appealing Simplicity Of The Standard Image Stream.  It Is No
%Longer Possible To Simply Unzip All The Chunks And Concatenate Them
%Together To Recover The Original Image.

\subsection{Encrypted Streams}
\label{crypted_stream}
Encryption is an important property in an evidence file format. In
particular, multiple streams may be present in the file set, and often
different access levels are desired. For example, for evidence set
containing both network captures and disk images it may be desirable
to limit access to streams based on legal authorizations, even though
the same set is distributed to a number of people.

Although the Zip64 standard specifies encryption, it is not suitable
for our purposes since it encrypts each segment separately, and does
not specify a sufficiently flexible scheme (e.g. support for PKI, PGP
keys). Segment based encryption may lead to information leakage when
segments are compressed, as the uncompressed size of the segment may
be deduced.

AFF4 therefore introduces a new encryption scheme, the Encrypted
Stream.  The Encrypted Stream provides transparent encryption and
decryption onto a single target stream. The target stream actually
stores the encrypted data, and read requests from the stream are
satisfied by decrypting the relevant data from this backing
stream. 

The Encrypted Stream may contain any data at all. It is useful
however, to store an actual AFF4 volume within the Encrypted
stream. This provides block level encryption for the contained AFF4
volume. It is recommended that Encrypted Streams containing an AFF4
volume set an attribute \texttt{content-type=application/x-aff-volume}
on the container stream, to allow implementations to automatically
recognize volumes. This approach is illustrated in
Figure~\ref{crypted_fif}.

\begin{figure}[tb]
  \begin{center}
  \mbox{\epsfxsize=0.8\columnwidth \epsffile{crypted.eps}}
  \caption{Embedding an encrypted AFF4 volume within an Encrypted
Stream. The container volume contains an encrypted stream backed by an
image stream. Once the encrypted stream is opened, the volume stored
on its image stream is accessible. Now it is possible to see the
secret image stream stored within the volume.}
  \label{crypted_fif}
  \end{center}
\end{figure}

The result is that a number of AFF4 volumes are used as {\em Container
Volumes} to provide storage for Encrypted Streams. The main {\em
Embedded Volume}, which actually contains data is stored within the
Encrypted Stream, effectively distributed throughout the container
volumes. Note that the outer Volume may contain several Encrypted
Streams and therefore contain multiple AFF4 Encrypted 
Volumes. Container Volumes may contain non encrypted streams as well,
and may implement different encryption schemes and keys for each
Encrypted scheme. This effectively allows arbitrary access policies to
be implemented as only volumes which can be accessed can be merged.

\subsection{Encryption Schemes}
Our implementation provides a number of encryption schemes.

\subsubsection{Null Encryption}
This scheme ({\em scheme=null}) specifies no encryption at all. This
may be useful for testing an implementation, but obviously provides
no real security.

\subsubsection{AES-SHA-PSK}
This scheme ({\em scheme=aes-sha-psk}) uses AES for block level
encryption, with the following SHA based ESSIV scheme, and Pre-Shared
Key (passphrase).

When creating a new encryption stream, a master key is generated using
the first 128 bits of a SHA1 hash of the Pre-Shared-Key appended to a
64 bit random salt:
\begin{eqnarray}
Key_{Master} = \left | SHA1(salt + PSK) \right | _{128}
\end{eqnarray}

For each chunk, the chunk IV is obtained by taking the first 128 bits
of the SHA1 hash of the chunk id encoded as a 32 bit little endian
integer, followed by the master key:
\begin{eqnarray}
IV = \left | SHA1(chunk\_id + Key_{Master}) \right | _{128}
\end{eqnarray}

The block is then encrypted or decrypted using AES in CBC mode with
this IV.

Implementations MUST specify the salt as a stream attribute named {\em
salt} in base 64 encoding. Implementations are free to specify any API
for passing the Pre-Shared Key, for example the PSK can be specified
in a configuration file, typed in at the terminal, passed in using
process environment variables, or encoded within the AFF4 filename
itself.

\subsection{Private Streams}
Often forensic software needs to store internal state, such as the
current state of the GUI, or internal data structures which may be
required for caching. Usually, software create additional cache files
to maintain this information. It is advantageous to be able to store
these in the evidence file itself. The evidence file can then be
re-opened by the software at a later stage and private data can be
retrieved and used directly. Such private application data can be
stored in a Private stream. There is no specification of what can be
stored in the private stream, and applications with private streams
should be able to store arbitrary segments in any format at all.

It is recommended that applications name their private segment
sufficiently accurately so as not to be confused with other
applications, or different versions of the same application
(e.g. ``pyflag/0.87/cache/'').


\section{Implementing AFF4 in AFFLIB}
Although there are numerous ZIP implementations available
today, we have created our own implementation using a combination of
available open source technology and our own novel contributions. We
are in the process of integrating this code into the AFFLIB library
and performing extensive regression testing. 

There are many reasons to develop our own ZIP64 implementation for
AFF4:

\begin{itemize}
\item Most ZIP implementations do not implement the ZIP64
  extensions. These extensions are required to support Evidence
  Volumes larger than 2GB.
\item Simple Zip implementations might rescan the
Central Directory for each segment request. Since in practice there
can be a large number of segments in a volume, it is advisable to have
a ZIP64 implementation that is optimized to storing thousands (or even
hundreds of thousands) of segments in an efficient data structure. 
\item While the ZIP specification duplicates data found in the Central
  Directory entry in each File Header (such as filename, size, CRC
  etc), many implementations that we have examined only populate this
  information in one of these places. In the interest of robustness,
  we wanted to assure that data stored in both locations would be
  populated to allow recovery of at least \emph{some} evidence that
  might exist in damaged volumes. If central directory is lost, it is
  possible to scan through the zip file and repair the central
  directory from the File headers.

\item Our implementation supports simultaneous access by multiple
  readers and writers. Our system allows concurrency by locking the
  file and interleaving write operations. Managing simultaneous
  writers is simplified through our update strategy which only writes
  new segments to the file, never modifying old segments. 

\item AFF4 ``properties'' have specific semantics above and beyond
  normal segments. For example, AFF4 calls for all of the property
  segments associated with a file to be merged together. Furthermore,
  when updating the properties in a multi-volume Set, it is useful to
  write the same properties to each volume to ensure at least some of
the stream can be used even if some volumes are lost. Such
functionality is not present in standard ZIP64 implementations.
\end{itemize}

\section{Usage Scenarios}
In this section we describe how a FIF file format may be used in
various situations. Many of those can be solved using other forensic
file formats, but often in a more awkward way.

\subsubsection{Rapidly converting a set of DD images}
Many hardware devices are available to acquire hard disks in the
fields. These often produce a set of uncompressed images split at a
certain size. It is possible to construct a Map Stream which
seamlessly reassembles the logical image from all the individual disk
images. The new stream may be kept in its own volume and may be very
small. 

If the images are split at exactly the same size its
possible to create a FIF archive consisting of a single Overlay
Stream, with a chunk\_size set to the size of each of the images. The
images can then be listed in order as external targets (e.g. {\em
target=file://disk1.dd}). This overlay can be created instantly
without needing to re-compress any of the images. In addition, digital
signatures can be generated for the whole stream or for each segment.

\subsubsection{Rapid conversion of EWF file sets}
Much existing evidence has been acquired using the EWF file
format. This format, similarly, consists of a series of chunks which
may be compressed, with interdispersed indexes. A tool such as
``ewf2fif'' is able to utilize the libewf library to create an Overlay
stream of the original image without the need to re-compress it. This
conversion is very quick and the resulting FIF file is very small -
each volume in the EWF file set is referenced through an external
target reference, and the Overlay Stream is built as an index of each
chunk location.

\subsubsection{Acquisition of RAID disks}
Often disks in a system are grouped into RAID devices, commonly RAID-5
or RAID-0. Previously, if disks were acquired independently, they
would need to be analysed using a tool which was able to reassemble
RAID devices.

With the FIF format, each of the disks can be acquired as a separate
Image Stream. Finally a tool such as PyFlag may be used to deduce the
RAID map, which can be appended to the FIF file as a Map Stream. This
Map Stream can then be opened by any tool without the tool needing to
have explicit support for RAID reassembling.

\subsubsection{Cryptographic management of evidence}
A FIF archive may hold multiple encrypted volumes, each in its own
Encrypted Stream. Each of those streams is encrypted using a different
master key, and therefore can have different passphrases, and can be
assigned to different users by encrypting the master key with
different X509 certificates. Its is also possible for users to create
non-encrypted volumes within the FIF file.

This can be used to enforce access controls in line with current
legislative requirements. For example, within the same investigation
different material is often obtained under different warrants
(e.g. wiretap authorizations are different from search
warrants). Therefore, different investigators and analysts need
different access to the different streams. However, the analysts may
still store the results of their analysis in an unencrypted form, or
assign others permissions to decrypt their analysis results, without
providing access to the underlying data. 

This can be used in sharing meta data (e.g. Map Streams of files of
interest) between analysts, without needing to provide access to the
underlying data.

\subsubsection{Forensic Application State}
Often forensic applications need to store files other than the
evidence itself - for example, they might need to store internal data,
annotations, keyword hits etc. Currently these applications store the
data in a proprietary external file or database. This makes it
difficult to archive because the evidence itself may become separated
from the case file.

It is advantageous for these applications to store their state within
the evidence file itself using a Private Stream. Then when opening the
evidence file again, the results of their analysis will become
available.

\subsubsection{Merging of evidence set}
In this common usage case, Bob, goes on site to image one hard disk,
while Alice independently images a second hard disk. After returning
to the office they both realise their images belong to the same
set. They merge their images, by updating the UUIDs on one of the
volume set to now belong to the other volume set - the merge is almost
instantaneous as they only need to write an extra ``properties'' file
on the merged volumes.

\section{Conclusion and Future Work}

\bibliography{IEEEabrv,paper}
\end{document}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

